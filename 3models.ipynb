{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb20e690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution status: Initating program\n",
      "Execution status: all libraries imported\n",
      "Execution status: transformations\n",
      "Execution status: defining training and evaluation\n",
      "Execution status: Dataset paths and labels loaded\n",
      "Execution status: Device set to cuda\n",
      "Execution status: Passed all prechecks, starting implementation\n",
      "\t MODE set to:  segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project_Space\\5.retina\\.venv\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "d:\\Project_Space\\5.retina\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Project_Space\\5.retina\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t step 1: Model initialized\n",
      "\t step 2: Optimizer initialized\n",
      "\t step 3: Dataset initialized\n",
      "\t step 4: Data loader initiated\n",
      "\t step 5: Initializing training loop\n",
      "\t\tEpoch 1/3: \n",
      "\t\t\tTraining Cycle\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 402\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m    401\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mTraining Cycle\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mEvaluating Cycle\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m     acc, dice = evaluate(model, val_dataloader, device,epoch)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 226\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader, optimizer, epoch, device)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m MODE == \u001b[33m\"\u001b[39m\u001b[33msegmentation\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    225\u001b[39m     outputs = model(images)\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     loss = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     outputs = torch.sigmoid(outputs)\n\u001b[32m    228\u001b[39m     dice_scores = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_Space\\5.retina\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_Space\\5.retina\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_Space\\5.retina\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:821\u001b[39m, in \u001b[36mBCEWithLogitsLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Project_Space\\5.retina\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3638\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3635\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3636\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m-> \u001b[39m\u001b[32m3638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m()):\n\u001b[32m   3639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3640\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3641\u001b[39m     )\n\u001b[32m   3643\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.binary_cross_entropy_with_logits(\n\u001b[32m   3644\u001b[39m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[32m   3645\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "#############################################################################################################################################\n",
    "#Chapter 1: presetups\n",
    "#############################################################################################################################################\n",
    "\n",
    "# section 1: importing libraries\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "print(\"Execution status: Initating program\")\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "print(\"Execution status: all libraries imported\")\n",
    "\n",
    "\n",
    "# section 2: seeding for reproducablity\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed()\n",
    "\n",
    "\n",
    "#############################################################################################################################################\n",
    "#Chapter 2: defintions\n",
    "#############################################################################################################################################\n",
    "\n",
    "# section x: saving visualization function\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "def save_visualization(image_tensor, pred_mask, true_mask, epoch, index):\n",
    "    image = image_tensor.cpu().permute(1, 2, 0).numpy()\n",
    "    pred = torch.sigmoid(pred_mask).cpu().numpy()\n",
    "    true = true_mask.cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[1].imshow(pred[0], cmap='Reds')\n",
    "    axs[1].set_title(\"Predicted Mask\")\n",
    "    axs[2].imshow(true[0], cmap='Greens')\n",
    "    axs[2].set_title(\"Ground Truth Mask\")\n",
    "    for ax in axs: ax.axis('off')\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    plt.savefig(f\"results/epoch_{epoch}_sample_{index}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# section n: Router\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, in_channels, num_experts=2):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, num_experts),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # Returns weights for each expert\n",
    "\n",
    "\n",
    "# section n: Dataset loader\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "class IDRiDDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, mask_dirs, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.mask_dirs = mask_dirs\n",
    "        self.mask_suffixes = ['_MA', '_HE', '_EX', '_SE', '_OD']\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        base_name = os.path.splitext(os.path.basename(self.image_paths[idx]))[0]\n",
    "\n",
    "        masks = []\n",
    "        for mask_dir, suffix in zip(self.mask_dirs, self.mask_suffixes):\n",
    "            mask_path = os.path.join(mask_dir, base_name + suffix + \".tif\")\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "            masks.append(np.array(mask))\n",
    "        mask_stack = np.stack(masks, axis=0)  # Shape: (5, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=np.array(image), mask=mask_stack.transpose(1, 2, 0))\n",
    "            image = augmented['image']\n",
    "            mask_stack = augmented['mask'].permute(2, 0, 1)  # Back to (5, H, W)\n",
    "        return image, torch.tensor(label, dtype=torch.long), mask_stack.float()\n",
    "print(\"Execution status: transformations\")\n",
    "\n",
    "\n",
    "\n",
    "# section n: data augumentation\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Rotate(limit=15, p=0.3),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.3),\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "# section n: Architecture definition - shared backbone\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "class SharedBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Remove avgpool and fc\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)  # Output shape: [B, 512, H/32, W/32]\n",
    "\n",
    "\n",
    "\n",
    "# section n: Architecture definition - classification\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "class ClassificationExpert(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# section n: Architecture definition - segmentation expert\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "class SegmentationExpert(nn.Module):\n",
    "    def __init__(self, in_channels=512, out_channels=5):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, 256, kernel_size=4, stride=2, padding=1),  # 8 → 16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),          # 16 → 32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),           # 32 → 64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),            # 64 → 128\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, out_channels, kernel_size=4, stride=2, padding=1),  # 128 → 256\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "# section n: Multi task model definition\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.shared = SharedBackbone()\n",
    "        self.router = Router(in_channels=512, num_experts=2)\n",
    "        self.classifier = ClassificationExpert(512, num_classes)\n",
    "        self.segmenter = SegmentationExpert()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)  # Shared feature extraction\n",
    "        routing_weights = self.router(features)  # Shape: [B, 2]\n",
    "        class_out = self.classifier(features)  # Shape: [B, num_classes]\n",
    "        seg_out = self.segmenter(features)     # Shape: [B, 5, H, W]\n",
    "        class_gate = routing_weights[:, 0].unsqueeze(1)  # Shape: [B, 1]\n",
    "        seg_gate = routing_weights[:, 1].unsqueeze(1).unsqueeze(2).unsqueeze(3)  # Shape: [B, 1, 1, 1]\n",
    "        gated_class_out = class_gate * class_out\n",
    "        gated_seg_out = seg_gate * seg_out\n",
    "        return gated_class_out, gated_seg_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# section n: Loss function\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "def multitask_loss(class_pred, class_target, seg_pred, seg_target, alpha=0.5):\n",
    "    classification_loss = nn.CrossEntropyLoss()(class_pred, class_target)\n",
    "    segmentation_loss = nn.BCEWithLogitsLoss()(seg_pred, seg_target)\n",
    "    return alpha * classification_loss + (1 - alpha) * segmentation_loss\n",
    "\n",
    "print(\"Execution status: defining training and evaluation\")\n",
    "\n",
    "\n",
    "# section n: traning\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "def train(model, dataloader, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    for batch_idx, (images, labels, masks) in enumerate(dataloader):\n",
    "        images, labels, masks = images.to(device), labels.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if MODE == \"classification\":\n",
    "            outputs = model(images)\n",
    "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            acc = (preds == labels).float().mean().item()\n",
    "            writer.add_scalar('Classification/Train_Loss', loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "            writer.add_scalar('Classification/Train_Accuracy', acc, epoch * len(dataloader) + batch_idx)\n",
    "\n",
    "        elif MODE == \"segmentation\":\n",
    "            outputs = model(images)\n",
    "            loss = nn.BCEWithLogitsLoss()(outputs, masks)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            dice_scores = []\n",
    "            for i in range(outputs.shape[1]):\n",
    "                intersection = (outputs[:, i] * masks[:, i]).sum()\n",
    "                union = outputs[:, i].sum() + masks[:, i].sum()\n",
    "                dice = (2. * intersection / (union + 1e-8)).item()\n",
    "                dice_scores.append(dice)\n",
    "            writer.add_scalar('Segmentation/Train_Loss', loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "            writer.add_scalar('Segmentation/Train_Dice', sum(dice_scores)/len(dice_scores), epoch * len(dataloader) + batch_idx)\n",
    "\n",
    "        elif MODE == \"multitask\":\n",
    "            class_out, seg_out = model(images)\n",
    "            loss = multitask_loss(class_out, labels, seg_out, masks)\n",
    "            preds = torch.argmax(class_out, dim=1)\n",
    "            acc = (preds == labels).float().mean().item()\n",
    "            seg_out = torch.sigmoid(seg_out)\n",
    "            dice_scores = []\n",
    "            for i in range(seg_out.shape[1]):\n",
    "                intersection = (seg_out[:, i] * masks[:, i]).sum()\n",
    "                union = seg_out[:, i].sum() + masks[:, i].sum()\n",
    "                dice = (2. * intersection / (union + 1e-8)).item()\n",
    "                dice_scores.append(dice)\n",
    "            writer.add_scalar('Multitask/Train_Loss', loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "            writer.add_scalar('Multitask/Train_Accuracy', acc, epoch * len(dataloader) + batch_idx)\n",
    "            writer.add_scalar('Multitask/Train_Dice', sum(dice_scores)/len(dice_scores), epoch * len(dataloader) + batch_idx)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    dice_scores = [[] for _ in range(5)]\n",
    "    class_correct = [0] * 5\n",
    "    class_total = [0] * 5\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, masks) in enumerate(dataloader):\n",
    "            images, labels, masks = images.to(device), labels.to(device), masks.to(device)\n",
    "\n",
    "            if MODE == \"classification\":\n",
    "                outputs = model(images)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                total_correct += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                acc = total_correct / total_samples\n",
    "                writer.add_scalar('Classification/Val_Loss', loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "                writer.add_scalar('Classification/Val_Accuracy', acc, epoch * len(dataloader) + batch_idx)\n",
    "\n",
    "            elif MODE == \"segmentation\":\n",
    "                outputs = model(images)\n",
    "                loss = nn.BCEWithLogitsLoss()(outputs, masks)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                for i in range(5):\n",
    "                    intersection = (outputs[:, i] * masks[:, i]).sum(dim=(1, 2))\n",
    "                    union = outputs[:, i].sum(dim=(1, 2)) + masks[:, i].sum(dim=(1, 2))\n",
    "                    dice = (2. * intersection / (union + 1e-8)).mean().item()\n",
    "                    dice_scores[i].append(dice)\n",
    "                writer.add_scalar('Segmentation/Val_Loss', loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "                writer.add_scalar('Segmentation/Val_Dice', np.mean([np.mean(d) for d in dice_scores]), epoch * len(dataloader) + batch_idx)\n",
    "\n",
    "            elif MODE == \"multitask\":\n",
    "                class_out, seg_out = model(images)\n",
    "                loss = multitask_loss(class_out, labels, seg_out, masks)\n",
    "                preds = torch.argmax(class_out, dim=1)\n",
    "                for i in range(len(labels)):\n",
    "                    class_total[labels[i].item()] += 1\n",
    "                    if preds[i].item() == labels[i].item():\n",
    "                        class_correct[labels[i].item()] += 1\n",
    "                total_correct += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                seg_out = torch.sigmoid(seg_out)\n",
    "                for i in range(5):\n",
    "                    intersection = (seg_out[:, i] * masks[:, i]).sum(dim=(1, 2))\n",
    "                    union = seg_out[:, i].sum(dim=(1, 2)) + masks[:, i].sum(dim=(1, 2))\n",
    "                    dice = (2. * intersection / (union + 1e-8)).mean().item()\n",
    "                    dice_scores[i].append(dice)\n",
    "                acc = total_correct / total_samples\n",
    "                writer.add_scalar('Multitask/Val_Loss', loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "                writer.add_scalar('Multitask/Val_Accuracy', acc, epoch * len(dataloader) + batch_idx)\n",
    "                writer.add_scalar('Multitask/Val_Dice', np.mean([np.mean(d) for d in dice_scores]), epoch * len(dataloader) + batch_idx)\n",
    "\n",
    "    return total_correct / total_samples, np.mean([np.mean(d) for d in dice_scores])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# section n: Save model\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "def save_model(model, path=\"saved_models/multitask_model.pth\"):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################################################################################\n",
    "#Chapter 3 Implementation\n",
    "#############################################################################################################################################\n",
    "\n",
    "# section 3: setting path of dataset and loading data\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "image_dir = r\"D:\\Project_Space\\5.retina\\retina_data\\B. Disease Grading\\1. Original Images\\a. Training Set\"\n",
    "label_csv = r\"D:\\Project_Space\\5.retina\\retina_data\\B. Disease Grading\\2. Groundtruths\\a. IDRiD_Disease Grading_Training Labels.csv\"\n",
    "mask_dirs = [\n",
    "    r\"D:\\Project_Space\\5.retina\\retina_data\\A. Segmentation\\2. All Segmentation Groundtruths\\a. Training Set\\1. Microaneurysms\",\n",
    "    r\"D:\\Project_Space\\5.retina\\retina_data\\A. Segmentation\\2. All Segmentation Groundtruths\\a. Training Set\\2. Haemorrhages\",\n",
    "    r\"D:\\Project_Space\\5.retina\\retina_data\\A. Segmentation\\2. All Segmentation Groundtruths\\a. Training Set\\3. Hard Exudates\",\n",
    "    r\"D:\\Project_Space\\5.retina\\retina_data\\A. Segmentation\\2. All Segmentation Groundtruths\\a. Training Set\\4. Soft Exudates\",\n",
    "    r\"D:\\Project_Space\\5.retina\\retina_data\\A. Segmentation\\2. All Segmentation Groundtruths\\a. Training Set\\5. Optic Disc\"\n",
    "]\n",
    "image_paths = sorted(glob.glob(image_dir + r\"\\*.jpg\"))\n",
    "label_df = pd.read_csv(label_csv)\n",
    "filename_to_label = dict(zip(label_df['Image name'], label_df['Retinopathy grade']))\n",
    "labels = [filename_to_label[os.path.splitext(os.path.basename(p))[0]] for p in image_paths]\n",
    "print(\"Execution status: Dataset paths and labels loaded\")\n",
    "\n",
    "val_image_dir = r\"D:\\Project_Space\\5.retina\\retina_data\\B. Disease Grading\\1. Original Images\\b. Testing Set\"\n",
    "val_label_csv = r\"D:\\Project_Space\\5.retina\\retina_data\\B. Disease Grading\\2. Groundtruths\\b. IDRiD_Disease Grading_Testing Labels.csv\"\n",
    "val_mask_dirs = [\n",
    "    r\"D:\\\\Project_Space\\\\5.retina\\\\retina_data\\\\A. Segmentation\\\\2. All Segmentation Groundtruths\\\\b. Testing Set\\\\1. Microaneurysms\",\n",
    "    r\"D:\\\\Project_Space\\\\5.retina\\\\retina_data\\\\A. Segmentation\\\\2. All Segmentation Groundtruths\\\\b. Testing Set\\\\2. Haemorrhages\",\n",
    "    r\"D:\\\\Project_Space\\\\5.retina\\\\retina_data\\\\A. Segmentation\\\\2. All Segmentation Groundtruths\\\\b. Testing Set\\\\3. Hard Exudates\",\n",
    "    r\"D:\\\\Project_Space\\\\5.retina\\\\retina_data\\\\A. Segmentation\\\\2. All Segmentation Groundtruths\\\\b. Testing Set\\\\4. Soft Exudates\",\n",
    "    r\"D:\\\\Project_Space\\\\5.retina\\\\retina_data\\\\A. Segmentation\\\\2. All Segmentation Groundtruths\\\\b. Testing Set\\\\5. Optic Disc\"\n",
    "]\n",
    "val_image_paths = sorted(glob.glob(val_image_dir + r\"\\\\*.jpg\"))\n",
    "val_label_df = pd.read_csv(val_label_csv)\n",
    "val_filename_to_label = dict(zip(val_label_df['Image name'], val_label_df['Retinopathy grade']))\n",
    "val_labels = [val_filename_to_label[os.path.splitext(os.path.basename(p))[0]] for p in val_image_paths]\n",
    "\n",
    "\n",
    "# section n: Choosing GPU\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Execution status: Device set to\", device)\n",
    "\n",
    "\n",
    "# section n: implementation\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "print(\"Execution status: Passed all prechecks, starting implementation\")\n",
    "MODE = 'segmentation' # options: 'multitask', 'classification', 'segmentation'\n",
    "\n",
    "print(\"\\t MODE set to: \", MODE)\n",
    "if MODE == \"classification\":\n",
    "    model = ClassificationExpert(in_channels=512, num_classes=5).to(device)\n",
    "elif MODE == \"segmentation\":\n",
    "    model = SegmentationExpert(in_channels=512, out_channels=5).to(device)\n",
    "elif MODE == \"multitask\":\n",
    "    model = MultiTaskModel(num_classes=5).to(device)\n",
    "else:\n",
    "    raise ValueError(\"Invalid MODE selected.\")\n",
    "model = MultiTaskModel(num_classes=5).to(device)\n",
    "dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "writer = SummaryWriter(log_dir=f\"runs/{MODE}_training\")\n",
    "writer.add_graph(model, dummy_input)\n",
    "print(\"\\t step 1: Model initialized\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "print(\"\\t step 2: Optimizer initialized\")\n",
    "dataset = IDRiDDataset(image_paths, labels, mask_dirs, transform=transform)\n",
    "val_dataset = IDRiDDataset(val_image_paths, val_labels, val_mask_dirs, transform=transform)\n",
    "print(\"\\t step 3: Dataset initialized\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "print(\"\\t step 4: Data loader initiated\")\n",
    "num_epochs =3\n",
    "print(\"\\t step 5: Initializing training loop\")\n",
    "writer = SummaryWriter(log_dir=\"runs/multitask_training\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\t\\tEpoch {epoch+1}/{num_epochs}: \\n\\t\\t\\tTraining Cycle\")\n",
    "    train(model, dataloader, optimizer, epoch, device)\n",
    "    print(f\"\\t\\t\\tEvaluating Cycle\")\n",
    "    acc, dice = evaluate(model, val_dataloader, device,epoch)\n",
    "    print(f\"\\t\\t\\t\\tAccuracy: {acc:.4f}, \\n\\t\\t\\t\\tDice Score: {dice:.4f}\")\n",
    "print(\"Execution status: saving model\")\n",
    "save_model(model)\n",
    "writer.close()\n",
    "print(\"Execution status: Program completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
