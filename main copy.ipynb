{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bf01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "#!pip3 install pandas\n",
    "#!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c435a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution status: all import statements successful\n",
      "Execution status: seeding completed\n",
      "Execution status: defining dataset class\n",
      "Execution status: transformations\n",
      "Execution status: model definitions\n",
      "Execution status: defining loss function\n",
      "Execution status: defining training and evaluation\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project_Space\\5.retina\\.venv\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "d:\\Project_Space\\5.retina\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Project_Space\\5.retina\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9294118..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4815, Dice Score: 0.4211\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5741, Dice Score: 0.4155\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.67058825..1.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0..1.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6111, Dice Score: 0.4119\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "print(\"Execution status: all import statements successful\")\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Setup and Paths\n",
    "# -------------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n",
    "print(\"Execution status: seeding completed\")\n",
    "\n",
    "# Update these paths to your local system\n",
    "image_dir = r\"D:\\Project_Space\\5.retina\\retina_data\\B. Disease Grading\\1. Original Images\\a. Training Set\"\n",
    "label_csv = r\"D:\\Project_Space\\5.retina\\retina_data\\B. Disease Grading\\2. Groundtruths\\a. IDRiD_Disease Grading_Training Labels.csv\"\n",
    "mask_dirs = [\n",
    "    r\"D:\\Project_Space\\5.retina\\retina_data\\A. Segmentation\\2. All Segmentation Groundtruths\\a. Training Set\\1. Microaneurysms\",\n",
    "    r\"D:\\Project_Space\\5.retina\\retina_data\\A. Segmentation\\2. All Segmentation Groundtruths\\a. Training Set\\2. Haemorrhages\",\n",
    "    r\"D:\\Project_Space\\5.retina\\retina_data\\A. Segmentation\\2. All Segmentation Groundtruths\\a. Training Set\\3. Hard Exudates\",\n",
    "    r\"D:\\Project_Space\\5.retina\\retina_data\\A. Segmentation\\2. All Segmentation Groundtruths\\a. Training Set\\4. Soft Exudates\",\n",
    "    r\"D:\\Project_Space\\5.retina\\retina_data\\A. Segmentation\\2. All Segmentation Groundtruths\\a. Training Set\\5. Optic Disc\"\n",
    "]\n",
    "\n",
    "image_paths = sorted(glob.glob(image_dir + r\"\\*.jpg\"))\n",
    "label_df = pd.read_csv(label_csv)\n",
    "filename_to_label = dict(zip(label_df['Image name'], label_df['Retinopathy grade']))\n",
    "labels = [filename_to_label[os.path.splitext(os.path.basename(p))[0]] for p in image_paths]\n",
    "\n",
    "print(\"Execution status: defining dataset class\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_visualization(image_tensor, pred_mask, true_mask, epoch, index):\n",
    "    image = image_tensor.cpu().permute(1, 2, 0).numpy()\n",
    "    pred = torch.sigmoid(pred_mask).cpu().numpy()\n",
    "    true = true_mask.cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[1].imshow(pred[0], cmap='Reds')\n",
    "    axs[1].set_title(\"Predicted Mask\")\n",
    "    axs[2].imshow(true[0], cmap='Greens')\n",
    "    axs[2].set_title(\"Ground Truth Mask\")\n",
    "    for ax in axs: ax.axis('off')\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    plt.savefig(f\"results/epoch_{epoch}_sample_{index}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, in_channels, num_experts=2):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, num_experts),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # Returns weights for each expert\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Dataset Class\n",
    "# -------------------------------\n",
    "class IDRiDDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, mask_dirs, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.mask_dirs = mask_dirs\n",
    "        self.mask_suffixes = ['_MA', '_HE', '_EX', '_SE', '_OD']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        base_name = os.path.splitext(os.path.basename(self.image_paths[idx]))[0]\n",
    "\n",
    "        masks = []\n",
    "        for mask_dir, suffix in zip(self.mask_dirs, self.mask_suffixes):\n",
    "            mask_path = os.path.join(mask_dir, base_name + suffix + \".tif\")\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "            masks.append(np.array(mask))\n",
    "\n",
    "        mask_stack = np.stack(masks, axis=0)  # Shape: (5, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=np.array(image), mask=mask_stack.transpose(1, 2, 0))\n",
    "            image = augmented['image']\n",
    "            mask_stack = augmented['mask'].permute(2, 0, 1)  # Back to (5, H, W)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long), mask_stack.float()\n",
    "\n",
    "print(\"Execution status: transformations\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Transformations\n",
    "# -------------------------------\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Rotate(limit=15, p=0.3),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.3),\n",
    "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "print(\"Execution status: model definitions\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model Definition\n",
    "# -------------------------------\n",
    "class SharedBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Remove avgpool and fc\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)  # Output shape: [B, 512, H/32, W/32]\n",
    "\n",
    "class ClassificationExpert(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class SegmentationExpert(nn.Module):\n",
    "    def __init__(self, in_channels=512, out_channels=5):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, 256, kernel_size=4, stride=2, padding=1),  # 8 → 16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),          # 16 → 32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),           # 32 → 64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),            # 64 → 128\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, out_channels, kernel_size=4, stride=2, padding=1),  # 128 → 256\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.shared = SharedBackbone()\n",
    "        self.router = Router(in_channels=512, num_experts=2)\n",
    "        self.classifier = ClassificationExpert(512, num_classes)\n",
    "        self.segmenter = SegmentationExpert()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)  # Shared feature extraction\n",
    "        routing_weights = self.router(features)  # Shape: [B, 2]\n",
    "\n",
    "        # Apply routing weights to gate expert outputs\n",
    "        class_out = self.classifier(features)  # Shape: [B, num_classes]\n",
    "        seg_out = self.segmenter(features)     # Shape: [B, 5, H, W]\n",
    "\n",
    "        # Gating mechanism: scale outputs by routing weights\n",
    "        class_gate = routing_weights[:, 0].unsqueeze(1)  # Shape: [B, 1]\n",
    "        seg_gate = routing_weights[:, 1].unsqueeze(1).unsqueeze(2).unsqueeze(3)  # Shape: [B, 1, 1, 1]\n",
    "\n",
    "        gated_class_out = class_gate * class_out\n",
    "        gated_seg_out = seg_gate * seg_out\n",
    "\n",
    "        return gated_class_out, gated_seg_out\n",
    "\n",
    "\n",
    "\n",
    "print(\"Execution status: defining loss function\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Loss Function\n",
    "# -------------------------------\n",
    "def multitask_loss(class_pred, class_target, seg_pred, seg_target, alpha=0.5):\n",
    "    classification_loss = nn.CrossEntropyLoss()(class_pred, class_target)\n",
    "    segmentation_loss = nn.BCEWithLogitsLoss()(seg_pred, seg_target)\n",
    "    return alpha * classification_loss + (1 - alpha) * segmentation_loss\n",
    "\n",
    "print(\"Execution status: defining training and evaluation\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Training and Evaluation\n",
    "# -------------------------------\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    with open(\"training_log.txt\", \"a\") as f:\n",
    "        f.write(\"f{epoch},{batch_idx},{loss.item():.4f}\\n\")\n",
    "    model.train()\n",
    "    with open(\"training_log.csv\", \"w\") as f:\n",
    "            f.write(\"epoch,batch,loss\\n\")\n",
    "    for images, labels, masks in dataloader:\n",
    "        images, labels, masks = images.to(device), labels.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        class_out, seg_out = model(images)\n",
    "        loss = multitask_loss(class_out, labels, seg_out, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    dice_scores = [[] for _ in range(5)]  # For each lesion type\n",
    "    class_correct = [0] * 5\n",
    "    class_total = [0] * 5\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, masks) in enumerate(dataloader):\n",
    "            images, labels, masks = images.to(device), labels.to(device), masks.to(device)\n",
    "            class_out, seg_out = model(images)\n",
    "\n",
    "            # Classification accuracy per class\n",
    "            preds = torch.argmax(class_out, dim=1)\n",
    "            for i in range(len(labels)):\n",
    "                class_total[labels[i].item()] += 1\n",
    "                if preds[i].item() == labels[i].item():\n",
    "                    class_correct[labels[i].item()] += 1\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Dice score per lesion\n",
    "            seg_out = torch.sigmoid(seg_out)\n",
    "            for i in range(5):  # 5 lesion types\n",
    "                intersection = (seg_out[:, i] * masks[:, i]).sum(dim=(1, 2))\n",
    "                union = seg_out[:, i].sum(dim=(1, 2)) + masks[:, i].sum(dim=(1, 2))\n",
    "                dice = (2. * intersection) / (union + 1e-8)\n",
    "                dice_scores[i].extend(dice.cpu().numpy())\n",
    "\n",
    "            # Save visualizations for first batch\n",
    "            if batch_idx == 0:\n",
    "                for i in range(min(2, images.size(0))):\n",
    "                    save_visualization(images[i], seg_out[i], masks[i], epoch, i)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = total_correct / total_samples\n",
    "    avg_dice = [sum(scores) / len(scores) for scores in dice_scores]\n",
    "    class_acc = [correct / total if total > 0 else 0.0 for correct, total in zip(class_correct, class_total)]\n",
    "\n",
    "    # Log metrics\n",
    "    with open(\"task_metrics_log.csv\", \"a\") as f:\n",
    "        f.write(f\"Epoch {epoch}, Overall Accuracy: {accuracy:.4f}, \" +\n",
    "                \", \".join([f\"Class{i}_Acc: {acc:.4f}\" for i, acc in enumerate(class_acc)]) + \", \" +\n",
    "                \", \".join([f\"Lesion{i}_Dice: {dice:.4f}\" for i, dice in enumerate(avg_dice)]) + \"\\n\")\n",
    "\n",
    "    return accuracy, sum(avg_dice) / len(avg_dice)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Main Execution\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MultiTaskModel(num_classes=5).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "dataset = IDRiDDataset(image_paths, labels, mask_dirs, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    train(model, dataloader, optimizer, device)\n",
    "    acc, dice = evaluate(model, dataloader, device,epoch)\n",
    "    print(f\"Accuracy: {acc:.4f}, Dice Score: {dice:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
